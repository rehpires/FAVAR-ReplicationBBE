{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.api import VAR\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e155267ce7387fe9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.read_excel(\"bbedata.xlsx\",index_col=0, parse_dates=True)\n",
    "df.index = df.index.strftime('%m/%Y')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1da59ae1374c4a8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(df)\n",
    "scaled_data = scaler.transform(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "533c9772ca8aebd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "slow_variables = df[[\"IP\", \"LHUR\", \"PUNEW\", \"IPP\", \"IPF\", \"IPC\", \"IPCD\", \"IPCN\", \"IPE\", \"IPI\", \"IPM\",\n",
    "                  \"IPMD\", \"IPMND\", \"IPMFG\", \"IPD\", \"IPN\", \"IPMIN\", \"IPUT\", \"IPXMCA\", \"PMI\", \"PMP\",\n",
    "                  \"GMPYQ\", \"GMYXPQ\", \"LHEL\", \"LHELX\", \"LHEM\", \"LHNAG\", \"LHU680\", \"LHU5\", \"LHU14\",\n",
    "                  \"LHU15\", \"LHU26\", \"LPNAG\", \"LP\", \"LPGD\", \"LPMI\", \"LPCC\", \"LPEM\", \"LPED\", \"LPEN\",\n",
    "                  \"LPSP\", \"LPTU\", \"LPT\", \"LPFR\", \"LPS\", \"LPGOV\", \"LPHRM\", \"LPMOSA\", \"PMEMP\", \"GMCQ\",\n",
    "                  \"GMCDQ\", \"GMCNQ\", \"GMCSQ\", \"GMCANQ\", \"PWFSA\", \"PWFCSA\", \"PWIMSA\", \"PWCMSA\", \"PSM99Q\",\n",
    "                  \"PU83\", \"PU84\", \"PU85\", \"PUC\", \"PUCD\", \"PUS\", \"PUXF\", \"PUXHS\", \"PUXM\", \"LEHCC\", \"LEHM\"]]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12473b95dedb7dc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calcula a média e o desvio padrão de cada variável e normaliza os dados.\n",
    "\n",
    "$$\n",
    "Z = \\frac{(X - U)}{S}\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "$Z$ é o valor normalizado,\n",
    "$X$ é o valor original,\n",
    "$U$ é a média dos valores originais,\n",
    "$S$ é o desvio padrão dos valores originais.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45b975253ac2e316"
  },
  {
   "cell_type": "code",
   "source": [
    "scaler.fit(slow_variables)\n",
    "scaled_slow_data = scaler.transform(slow_variables)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a831bf5b4a959691",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#dataframe of scaled data\n",
    "data_s = pd.DataFrame(scaled_data)\n",
    "data_s.columns = df.columns\n",
    "data_s.index = df.index\n",
    "\n",
    "FYFF = data_s[\"FYFF\"].reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93082051d220da67",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# PCA on entire dataset is C\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "C = pd.DataFrame(pca.fit_transform(scaled_data))\n",
    "\n",
    "# rename columns to PC1, PC2, PC3\n",
    "C.columns = [\"PC1\", \"PC2\", \"PC3\"]\n",
    "C[\"PC3\"] = C[\"PC3\"] * - 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "210e15b398d9dac6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "F_slow = pd.DataFrame(pca.fit_transform(scaled_slow_data))\n",
    "F_slow.columns = [\"F1\", \"F2\", \"F3\"]\n",
    "\n",
    "F_slow[\"FYFF\"] = FYFF\n",
    "\n",
    "# merge C and df2\n",
    "F_slow = pd.concat([C, F_slow], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7913878f743a98c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "A equação (1) é conhecida como Vetores autoregressivos aumentado por Fatores, FAVAR\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "F_t \\\\\n",
    "Y_t \n",
    "\\end{bmatrix}\n",
    "= \\Phi(L)\n",
    "\\begin{bmatrix}\n",
    "F_{t-1} \\\\\n",
    "Y_{t-1}\n",
    "\\end{bmatrix}\n",
    "+ V_t\n",
    "$$\n",
    "\n",
    "Onde: \n",
    "$F_t$ é um vetor de fatores não observados, \n",
    "$Y_t$ é um vetor de variáveis observadas, \n",
    "$\\Phi(L)$ é uma matriz de polinômios de defasagem,\n",
    "$V_t$ é um vetor de resíduos. \n",
    "\n",
    "O vetor $Y_t$ contém $M$ variáveis econômicas observáveis.\n",
    "O vetor $F_t$ contém $K$ fatores não observados que supostamente influenciam as variáveis econômicas;"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b726182593c522ff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "A Equação (1) não pode ser estimada diretamente uma vez que os fatores $\\( F_t \\)$ são inobserváveis e devem ser substituídos por $\\( \\hat{F}_t \\)$.\n",
    "Os fatores estimados, $\\( \\hat{F}_t \\)$, são assumidos baseados em um número de séries temporais que coletivamente são denotadas pelo vetor $\\( N \\times 1 \\) \\( X_t \\)$.\n",
    "Qualquer economia desenvolvida envolve muitas atividades diferentes que podem ser descritas por várias séries temporais.\n",
    "Assim, o número de séries temporais $\\( N \\)$ em $\\( X_t \\)$ é considerado grande, e pode ser bem maior que $\\( T \\)$, o número de períodos de tempo.\n",
    "Bernanke et al. (2005) assumem que as séries temporais em $\\( X_t \\)$ estão relacionadas aos fatores inobserváveis $\\( F_t \\)$ e às variáveis econômicas observáveis $\\( Y_t \\)$ por uma equação dada por\n",
    "\n",
    "$$\n",
    "\\[\n",
    "    X_t = \\Lambda F_t + \\Lambda^Y Y_t + e_t\n",
    "\\]\n",
    "$$\n",
    "Onde $\\( \\Lambda \\)$ é uma matriz $\\( N \\times K \\)$ de fatores carregados,\n",
    "$\\( \\Lambda^Y \\)$ tem dimensão $\\( N \\times M \\)$\n",
    "$\\( e_t \\)$ é $\\( N \\times 1 \\)$ um vetor de erros com média zero.\n",
    "\n",
    "A equação (2) expressa a ideia de que tanto $\\( Y_t \\)$ como $\\( F_t \\)$, que podem ser correlacionados, expressão as relações contidas em $\\( X_t \\)$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90c28188480d1708"
  },
  {
   "cell_type": "code",
   "source": [
    "Factor = pd.DataFrame()\n",
    "\n",
    "def perform_ols_and_update_factor(dependent_var):\n",
    "    y = C[[dependent_var]]\n",
    "    x = F_slow[[\"FYFF\", \"F1\", \"F2\", \"F3\"]]\n",
    "    x = sm.add_constant(x)\n",
    "    model = sm.OLS(y, x).fit()\n",
    "    F_hat = C - np.dot(data_s[\"FYFF\"].values.reshape(-1, 1), model.params[1])\n",
    "    Factor[dependent_var] = F_hat[dependent_var]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9caaf91da490c1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Call the function for each dependent variable\n",
    "perform_ols_and_update_factor(\"PC1\")\n",
    "perform_ols_and_update_factor(\"PC2\")\n",
    "perform_ols_and_update_factor(\"PC3\")\n",
    "\n",
    "Factor[\"FYFF\"] = data_s[\"FYFF\"].reset_index(drop=True)\n",
    "df_var = Factor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9e91dd38a41806a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "F_hat = Factor[[\"PC1\", \"PC2\", \"PC3\",\"FYFF\"]].values\n",
    "loadings = pd.DataFrame(np.linalg.lstsq(F_hat, data_s, rcond=None)[0])\n",
    "loadings.columns = df.columns\n",
    "loadings.index = [\"PC1\", \"PC2\", \"PC3\",\"FYFF\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fac73fefcef98902",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "reg_loadings = np.linalg.lstsq(F_hat, data_s, rcond=None)\n",
    "\n",
    "# get fitted values\n",
    "fitted_values = np.dot(F_hat, reg_loadings[0])\n",
    "fitted_values = pd.DataFrame(fitted_values)\n",
    "fitted_values.columns = data_s.columns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e750b8d9cd290cc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inclusão dos Fatores no VAR: \n",
    "Uma vez estimados, esses fatores são incluídos como variáveis adicionais no modelo VAR, \n",
    "permitindo que o modelo capture as dinâmicas mais amplas da economia que não seriam capturadas apenas pelas variáveis observáveis.\n",
    "\n",
    "A equação (1) do modelo FAVAR, agora com os fatores devidamente estimados pode ser expressa pela equação (3):\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\( \\hat{F}_t \\)\\\\\n",
    "Y_t \n",
    "\\end{bmatrix}\n",
    "= \\Phi(L)\n",
    "\\begin{bmatrix}\n",
    "\\( \\hat{F}_{t-1} \\) \\\\\n",
    "Y_{t-1}\n",
    "\\end{bmatrix}\n",
    "+ V_t\n",
    "$$\n",
    "\n",
    "Onde: \n",
    "$\\hat{F}_t$ é um vetor de fatores estimados pela equação (2), \n",
    "$Y_t$ é um vetor de variáveis observadas, \n",
    "$\\Phi(L)$ é uma matriz de polinômios de defasagem,\n",
    "$V_t$ é um vetor de resíduos. \n",
    "\n",
    "O vetor $Y_t$ contém $M$ variáveis econômicas observáveis.\n",
    "O vetor $\\hat{F}_t$ contém $K$ fatores estimados a partir das séries temporais que influenciam a economia;\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ee73e5c54320c31"
  },
  {
   "cell_type": "code",
   "source": [
    "# O modelo FAVAR estimado é dado por:\n",
    "\n",
    "model = VAR(df_var)\n",
    "results = model.fit(13, trend=\"c\")\n",
    "resid = results.resid\n",
    "params = results.params\n",
    "lag_order = results.k_ar\n",
    "\n",
    "orth_irf = results.irf(48).orth_irfs\n",
    "irf_fyff = orth_irf[:, :, results.names.index('FYFF')]\n",
    "irf_fyff = pd.DataFrame(irf_fyff)\n",
    "irf_fyff.index = range(0, 49)\n",
    "\n",
    "# Calculando o desvio padrão do impulso e escalando o IRF\n",
    "impulse_sd = 0.25 / df['FYFF'].std()\n",
    "scale = impulse_sd / irf_fyff[3].iloc[0]\n",
    "\n",
    "irf_line = np.dot(irf_fyff, loadings.iloc[:4]) * scale"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24424e43664be6d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "R = 500\n",
    "nvars = 120\n",
    "nsteps = 49\n",
    "\n",
    "IRFs = np.zeros((nsteps, nvars, R))\n",
    "Upper = np.zeros((nsteps, nvars))\n",
    "Lower = np.zeros((nsteps, nvars))\n",
    "const_in_model = 'const' in results.exog_names\n",
    "\n",
    "# Precompute as much as possible outside the loop\n",
    "lagged_data = np.hstack([df_var.shift(lag).values[lag_order:] for lag in range(1, lag_order + 1)])\n",
    "\n",
    "# This part assumes parallel processing or similar optimization is applied\n",
    "for j in range(R):\n",
    "    resampled_resid = resid.sample(n=len(resid), replace=True, random_state=j).reset_index(drop=True)\n",
    "    data_boot = df_var.copy()\n",
    "\n",
    "    for i in range(lag_order, len(df_var)):\n",
    "        lagged_values = np.concatenate([data_boot.iloc[i - lag].values for lag in range(1, lag_order + 1)])\n",
    "\n",
    "        if const_in_model:\n",
    "            lagged_values = np.insert(lagged_values, 0, 1)\n",
    "\n",
    "        predicted_value = np.dot(lagged_values, results.params) + resampled_resid.iloc[i - lag_order]\n",
    "        data_boot.iloc[i] = predicted_value\n",
    "\n",
    "    model_boot = VAR(data_boot)\n",
    "    results_boot = model_boot.fit(13, trend='c')\n",
    "\n",
    "    irf_res = results_boot.irf(48).orth_irfs\n",
    "    for i in range(nvars):\n",
    "        irf_transformed = np.dot(irf_res[:, :, results_boot.names.index('FYFF')], loadings.iloc[:4, i]) * scale\n",
    "        IRFs[:, i, j] = irf_transformed[:nsteps]\n",
    "\n",
    "for k in range(nsteps):\n",
    "    for i in range(nvars):\n",
    "        Upper[k, i] = np.quantile(IRFs[k, i, :], 0.95)\n",
    "        Lower[k, i] = np.quantile(IRFs[k, i, :], 0.05)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "771d10da4aab3e31",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Plotando os IRFs\n",
    "variables = [\"FYFF\", \"IP\", \"PUNEW\", \"FYGM3\", \"FYGT5\", \"FMFBA\", \n",
    "             \"FM2\", \"EXRJAN\", \"PMCP\", \"IPXMCA\", \"GMCQ\", \"GMCDQ\", \n",
    "             \"GMCNQ\", \"LHUR\", \"PMEMP\", \"LEHM\", \"HSFR\", \"PMNO\", \"FSDXP\", \"HHSNTN\"]\n",
    "\n",
    "transf_code = [1, 5, 5, 1, 1, 5, 5, 5, 1, 1, 5, 5, 5, 1, 1, 5, 1, 1, 1, 1]\n",
    "\n",
    "variable_names = [\"Fed Funds Rate\", \"Industrial Production\", \"CPI\", \"3m Treasury Bills\",\n",
    "                  \"5y Treasury Bonds\", \"Monetary Base\", \"M2\", \"Exchange Rate Yen\", \n",
    "                  \"Commodity Price Index\", \"Capacity Util Rate\", \"Personal Consumption\", \n",
    "                  \"Durable Cons\", \"Nondurable Cons\", \"Unemployment\", \"Employment\", \n",
    "                  \"Avg Hourly Earnings\", \"Housing Starts\", \"New Orders\", \"Dividends\", \"Consumer Expectations\"]\n",
    "\n",
    "# Create a 5x4 grid of subplots and set the figure size in pixels\n",
    "fig, axs = plt.subplots(5, 4, figsize=(20, 12))\n",
    "\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "\n",
    "# Iterate over each variable\n",
    "for i, var in enumerate(variables):\n",
    "    # Get the index of the variable in the dataframe\n",
    "    index = data_s.columns.get_loc(var)\n",
    "\n",
    "    # Get the subplot axes\n",
    "    ax = axs[i // 4, i % 4]\n",
    "\n",
    "    # Plot the IRF, cumulative sum if transf_code is 5\n",
    "    if transf_code[i] == 5:\n",
    "        ax.plot(irf_line[:, index].cumsum(), label='IRF')\n",
    "        ax.plot(Upper[:, index].cumsum(), label='Upper', linestyle='--', color='red')\n",
    "        ax.plot(Lower[:, index].cumsum(), label='Lower', linestyle='--', color='red')\n",
    "    else:\n",
    "        ax.plot(irf_line[:, index], label='IRF')\n",
    "        ax.plot(Upper[:, index], label='Upper', linestyle='--', color='red')\n",
    "        ax.plot(Lower[:, index], label='Lower', linestyle='--', color='red')\n",
    "\n",
    "    # Set the title and labels\n",
    "    ax.set_title(variable_names[i])\n",
    "    ax.set_xlabel('Steps')\n",
    "    ax.set_ylabel('')\n",
    "\n",
    "    # Add a horizontal line at y=0\n",
    "    ax.axhline(0, color='black', linewidth=0.5)\n",
    "\n",
    "# Add a legend\n",
    "axs[0, 0].legend()\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "#save fig as png\n",
    "#fig.savefig('IRF_FAVAR.png', dpi=600)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42bb94942484c40d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Quanto da variação é capturado pelos fatores e pelo FYFF?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5803e44dcfd1a2fe"
  },
  {
   "cell_type": "code",
   "source": [
    "data_s.index = pd.to_datetime(data_s.index, format='%m/%Y')\n",
    "\n",
    "var = \"IPP\"\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.plot(data_s.index, data_s[var], label=var, color='blue', linestyle='dashed')\n",
    "ax.plot(data_s.index, fitted_values[var], label=var, color='red',linestyle='solid')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator(10))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae1174f9e4620301",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Decomposição da Variância de Erro de Previsão (FEVD)\n",
    "\n",
    "A decomposição da variância do erro de previsão (FEVD) é uma ferramenta útil para entender a importância relativa de cada variável em explicar a variância de outras variáveis no sistema.\n",
    "A decomposição da variância indica quanto a informação de cada variáveil contribui para a variância de cada variável no sistema, indicando quando do erro de previsão pode ser explicado por choque das outras variáveis."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "961cfa7c856cc4c4"
  },
  {
   "cell_type": "code",
   "source": [
    "# Get the IRF for 'hor' periods ahead without bootstrapping\n",
    "hor = 60\n",
    "\n",
    "orth_irf = results.irf(hor).orth_irfs\n",
    "\n",
    "# Initialize dictionaries to hold the IRFs\n",
    "irf_arrays = {\n",
    "    'irf_X_pc1': np.zeros((hor+1, len(variables))),\n",
    "    'irf_X_pc2': np.zeros((hor+1, len(variables))),\n",
    "    'irf_X_pc3': np.zeros((hor+1, len(variables))),\n",
    "    'irf_X_fyff': np.zeros((hor+1, len(variables)))\n",
    "}\n",
    "\n",
    "# Calculate the IRFs for each variable and loadings\n",
    "for i, var in enumerate(variables):\n",
    "    irf_arrays['irf_X_pc1'][:, i] = np.dot(orth_irf[:, :, results.names.index(\"PC1\")], loadings.iloc[:4, data_s.columns.get_loc(var)])\n",
    "    irf_arrays['irf_X_pc2'][:, i] = np.dot(orth_irf[:, :, results.names.index('PC2')], loadings.iloc[:4, data_s.columns.get_loc(var)])\n",
    "    irf_arrays['irf_X_pc3'][:, i] = np.dot(orth_irf[:, :, results.names.index('PC3')], loadings.iloc[:4, data_s.columns.get_loc(var)])\n",
    "    irf_arrays['irf_X_fyff'][:, i] = np.dot(orth_irf[:, :, results.names.index('FYFF')], loadings.iloc[:4, data_s.columns.get_loc(var)])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a5917d687edc549",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Assuming data_s is your response matrix and F_hat includes the predictors\n",
    "coefficients, residuals, rank, s = np.linalg.lstsq(F_hat, data_s, rcond=None)\n",
    "# Number of observations (n) and number of predictors (k)\n",
    "n = data_s.shape[0]\n",
    "k = F_hat.shape[1] - 1 \n",
    "\n",
    "# Subtracting 1 because we don't count the constant/intercept\n",
    "RSE = np.zeros(len(residuals))\n",
    "    \n",
    "for i in range(len(residuals)):\n",
    "    if residuals.size == 0:\n",
    "        RSE[i] = 0\n",
    "    else:\n",
    "        RSE[i] = np.sqrt(residuals[i] / (n - k - 1))\n",
    "        \n",
    "#set RSE index as data_s columns\n",
    "RSE = pd.DataFrame(RSE)\n",
    "RSE.index = data_s.columns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ab67fe03f207f99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "coefficients",
   "id": "1f76afe8ea67740e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate predicted values\n",
    "predicted_values = F_hat.dot(coefficients)\n",
    "\n",
    "# Calculate individual residuals\n",
    "individual_residuals = data_s - predicted_values\n",
    "\n",
    "# Calculate SSR (Sum of Squares of Residuals)\n",
    "SSR = np.sum(individual_residuals**2, axis=0)\n",
    "\n",
    "# Calculate SST (Total Sum of Squares)\n",
    "mean_data_s = np.mean(data_s, axis=0)\n",
    "SST = np.sum((data_s - mean_data_s)**2, axis=0)\n",
    "\n",
    "# Calculate R-squared for each regression\n",
    "R_squared = 1 - (SSR / SST)\n",
    "\n",
    "print(\"R-squared values:\", R_squared)"
   ],
   "id": "920e3268597af610",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize arrays to store squared IRFs\n",
    "\n",
    "hor = 60\n",
    "\n",
    "key_nvars = len(variables)\n",
    "\n",
    "psi2_pc1 = np.zeros(key_nvars)\n",
    "psi2_pc2 = np.zeros(key_nvars)\n",
    "psi2_pc3 = np.zeros(key_nvars)\n",
    "psi2_fyff = np.zeros(key_nvars)\n",
    "\n",
    "# Accumulate squared IRFs\n",
    "for i in range(key_nvars):  # Python is 0-indexed\n",
    "    for j in range(hor):\n",
    "        psi2_pc1[i] += irf_arrays[\"irf_X_pc1\"][j, i]**2\n",
    "        psi2_pc2[i] += irf_arrays[\"irf_X_pc2\"][j, i]**2\n",
    "        psi2_pc3[i] += irf_arrays[\"irf_X_pc3\"][j, i]**2\n",
    "        psi2_fyff[i] += irf_arrays[\"irf_X_fyff\"][j, i]**2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1a6241469c70889",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize arrays for total variance and variance from factors\n",
    "var_total = np.zeros(key_nvars)\n",
    "var_fac = np.zeros(key_nvars)\n",
    "var_e = np.zeros(key_nvars)\n",
    "\n",
    "# Calculate variances\n",
    "for i, var_name in enumerate(variables):\n",
    "    var_fac[i] = psi2_pc1[i] + psi2_pc2[i] + psi2_pc3[i] + psi2_fyff[i]\n",
    "\n",
    "    # Ensure you are accessing the RSE value for the current variable\n",
    "    rse_value = RSE.loc[var_name] ** 2  # This will get the RSE value for the specific variable\n",
    "\n",
    "    var_total[i] = var_fac[i] + rse_value\n",
    "    var_e[i] = rse_value"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f34fe765103c1fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#group psi2_pc1, psi2_pc2, psi2_pc3 and psi2_fyff in a dataframe\n",
    "irf_squared = pd.DataFrame({'psi2_pc1': psi2_pc1, 'psi2_pc2': psi2_pc2, 'psi2_pc3': psi2_pc3, 'psi2_fyff': psi2_fyff}, index=variables)\n",
    "irf_squared[\"var_e\"] = var_e\n",
    "irf_squared[\"var_fac\"] = var_fac\n",
    "irf_squared[\"var_total\"] = var_total\n",
    "\n",
    "#round columns to 4 decimal places\n",
    "irf_squared = irf_squared.round(3)\n",
    "irf_squared"
   ],
   "id": "b7d82c02107fd22a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#group var_fac, var_e and var_total in a dataframe\n",
    "variance_decomposition = pd.DataFrame({'var_fac': var_fac, 'var_e': var_e, 'var_total': var_total}, index=variables)\n",
    "variance_decomposition"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d45c332bc0f47aa4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "contribution = irf_squared[[\"psi2_fyff\"]].div(irf_squared[\"var_total\"], axis=0)\n",
    "contribution = contribution.round(3)\n",
    "contribution[\"R-squared\"] = R_squared\n",
    "contribution"
   ],
   "id": "5c1cb1dd876a4223",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
